## 1_28
- 现在训练集用的val_mini所以最多并行环境只有3个，现在用的是3个
- 4卡同时训练
- 评估时候也要改数据集名称

## 1_31
- 修改launch让参数默认话从而可以调试。launch只是用来并行的初始化文件，所以应该直接跑run.py
- _collect_rollout_step 中与环境交互
- ddppo-trainer.py中train进行更新

## 2_4
- 推理和训练流程基本上搞清楚了，现在测试下GRU的性能怎么样
- 把CHECKPOINT_INTERVAL调大
- 要改rollout?
- 改策略中的视觉编码器部分和状态编码器
- 他现在是step是连着的，我如何能做到切割原来的step为动态的呢，这样效率会不会降低呢，会的，不同环境的
  每个episode数量是不一样的所以不能这样直接切割。
  - 现在的训练流程是收集完成训练
  - 最重要的是要在state_encoder中加入mask_hidden, 还不够，因为如果中间被截断之前的状态就看不到了。
    但是如果长度不统一就不能并行同时采集多个环境了。。但是可以4张卡同时用也不是不行？ddppo本身支持长度不同时候的截断？
  - 目前的训练框架难以捕捉到完整的trajectory，所以不一定适合transformer
    - 要修改目前的训练框架从而获得完整的训练轨迹。两种修改方式，并行探索再遍历训练。还有可以对短的轨迹进行补零操作
  - 如果实在不行可以先只采集一个环境的信息